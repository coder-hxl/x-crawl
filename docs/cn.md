# x-crawl [![npm](https://img.shields.io/npm/v/x-crawl.svg)](https://www.npmjs.com/package/x-crawl) [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/coder-hxl/x-crawl/blob/main/LICENSE)

[English](https://github.com/coder-hxl/x-crawl#x-crawl) | ç®€ä½“ä¸­æ–‡

x-crawl æ˜¯ä¸€ä¸ªçµæ´»çš„ nodejs çˆ¬è™«åº“ã€‚å¯æ‰¹é‡çˆ¬å–é¡µé¢ã€æ‰¹é‡ç½‘ç»œè¯·æ±‚ã€æ‰¹é‡ä¸‹è½½æ–‡ä»¶èµ„æºã€è½®è¯¢çˆ¬å–ç­‰ã€‚æ”¯æŒ å¼‚æ­¥/åŒæ­¥ æ¨¡å¼çˆ¬å–ã€‚è·‘åœ¨ nodejs ä¸Šï¼Œç”¨æ³•çµæ´»å’Œç®€å•ï¼Œå¯¹ JS/TS å¼€å‘è€…å‹å¥½ã€‚

> å¦‚æœæ„Ÿè§‰ä¸é”™ï¼Œå¯ä»¥ç»™ [x-crawl å­˜å‚¨åº“](https://github.com/coder-hxl/x-crawl) ç‚¹ä¸ª Star æ”¯æŒä¸€ä¸‹ï¼Œæ‚¨çš„ Star å°†æ˜¯æˆ‘æ›´æ–°çš„åŠ¨åŠ›ã€‚

## ç‰¹å¾

- **ğŸ”¥ å¼‚æ­¥/åŒæ­¥** - æ”¯æŒ å¼‚æ­¥/åŒæ­¥ æ¨¡å¼æ‰¹é‡çˆ¬å–ã€‚
- **âš™ï¸ å¤šç§åŠŸèƒ½** - å¯æ‰¹é‡çˆ¬å–é¡µé¢ã€æ‰¹é‡ç½‘ç»œè¯·æ±‚ã€æ‰¹é‡ä¸‹è½½æ–‡ä»¶èµ„æºã€è½®è¯¢çˆ¬å–ç­‰ã€‚
- **ğŸ–‹ï¸ å†™æ³•çµæ´»** - å¤šç§çˆ¬å–é…ç½®ã€è·å–çˆ¬å–ç»“æœçš„å†™æ³•ã€‚
- **â±ï¸ é—´éš”çˆ¬å–** - æ— é—´éš”/å›ºå®šé—´éš”/éšæœºé—´éš”ï¼Œå¯ä»¥ ä½¿ç”¨/é¿å… é«˜å¹¶å‘çˆ¬å–ã€‚
- **â˜ï¸ çˆ¬å– SPA** - æ‰¹é‡çˆ¬å– SPAï¼ˆå•é¡µåº”ç”¨ç¨‹åºï¼‰ç”Ÿæˆé¢„æ¸²æŸ“å†…å®¹ï¼ˆå³â€œSSRâ€ï¼ˆæœåŠ¡å™¨ç«¯æ¸²æŸ“ï¼‰ï¼‰ã€‚
- **âš’ï¸ æ§åˆ¶é¡µé¢** - æ— å¤´æµè§ˆå™¨å¯ä»¥è¡¨å•æäº¤ã€é”®ç›˜è¾“å…¥ã€äº‹ä»¶æ“ä½œã€ç”Ÿæˆé¡µé¢çš„å±å¹•æˆªå›¾ç­‰ã€‚
- **ğŸ§¾ æ•è·è®°å½•** - å¯¹çˆ¬å–çš„ç»“æœè¿›è¡Œæ•è·è®°å½•ï¼Œå¹¶è¿›è¡Œé«˜äº®çš„æé†’ã€‚
- **ğŸ¦¾TypeScript** - æ‹¥æœ‰ç±»å‹ï¼Œé€šè¿‡æ³›å‹å®ç°å®Œæ•´çš„ç±»å‹ã€‚

## è·Ÿ puppeteer çš„å…³ç³»

crawlPage API å†…éƒ¨ä½¿ç”¨ [puppeteer](https://github.com/puppeteer/puppeteer) åº“æ¥å¸®åŠ©æˆ‘ä»¬çˆ¬å–é¡µé¢ï¼Œå¹¶å°† Brower å®ä¾‹å’Œ Page å®ä¾‹æš´éœ²å‡ºæ¥ã€‚

# ç›®å½•

- [å®‰è£…](#å®‰è£…)
- [ç¤ºä¾‹](#ç¤ºä¾‹)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
  - [åˆ›å»ºåº”ç”¨](#åˆ›å»ºåº”ç”¨)
    - [ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹](#ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹)
    - [é€‰æ‹©çˆ¬å–æ¨¡å¼](#é€‰æ‹©çˆ¬å–æ¨¡å¼)
    - [å¤šä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹](#å¤šä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹)
  - [çˆ¬å–é¡µé¢](#çˆ¬å–é¡µé¢)
    - [browser å®ä¾‹](#browser-å®ä¾‹)
    - [page å®ä¾‹](#page-å®ä¾‹)
  - [çˆ¬å–æ¥å£](#çˆ¬å–æ¥å£)
  - [çˆ¬å–æ–‡ä»¶](#çˆ¬å–æ–‡ä»¶)
  - [å¯åŠ¨è½®è¯¢](#å¯åŠ¨è½®è¯¢)
  - [çˆ¬å–é—´éš”æ—¶é—´](#çˆ¬å–é—´éš”æ—¶é—´)
  - [requestConfig é€‰é¡¹çš„å¤šç§å†™æ³•](#requestConfig-é€‰é¡¹çš„å¤šç§å†™æ³•)
  - [è·å–ç»“æœçš„å¤šç§æ–¹å¼](#è·å–ç»“æœçš„å¤šç§æ–¹å¼)
- [API](#API)
  - [xCrawl](#xCrawl)
    - [ç±»å‹](#ç±»å‹)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-1)
  - [crawlPage](#crawlPage)
    - [ç±»å‹](#ç±»å‹-1)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-2)
  - [crawlData](#crawlData)
    - [ç±»å‹](#ç±»å‹-2)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-3)
  - [crawlFile](#crawlFile)
    - [ç±»å‹](#ç±»å‹-3)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-4)
  - [startPolling](#startPolling)
    - [ç±»å‹](#ç±»å‹-4)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-5)
- [ç±»å‹](#ç±»å‹-5)
  - [AnyObject](#AnyObject)
  - [Method](#Method)
  - [RequestConfigObjectV1](#RequestConfigObjectV1)
  - [RequestConfigObjectV2](#RequestConfigObjectV2)
  - [RequestConfig](#RequestConfig)
  - [IntervalTime](#IntervalTime)
  - [XCrawlBaseConfig](#XCrawlBaseConfig)
  - [CrawlBaseConfigV1](#CrawlBaseConfigV1)
  - [CrawlBaseConfigV2](#CrawlBaseConfigV2)
  - [CrawlPageConfig](#CrawlPageConfig)
  - [CrawlDataConfig](#CrawlDataConfig)
  - [CrawlFileConfig](#CrawlFileConfig)
  - [StartPollingConfig](#StartPollingConfig)
  - [XCrawlInstance](#XCrawlInstance)
  - [CrawlResCommonV1](#CrawlResCommonV1)
  - [CrawlResCommonArrV1](#CrawlResCommonArrV1)
  - [CrawlPage](#CrawlPage-1)
  - [FileInfo](#FileInfo)
- [æ›´å¤š](#æ›´å¤š)

## å®‰è£…

ä»¥ NPM ä¸ºä¾‹:

```shell
npm install x-crawl
```

## ç¤ºä¾‹

æ¯å¤©è‡ªåŠ¨è·å– bilibili å›½æ¼«ä¸»é¡µçš„è½®æ’­å›¾ç‰‡ä¸ºä¾‹:

```js
// 1.å¯¼å…¥æ¨¡å— ES/CJS
import xCrawl from 'x-crawl'

// 2.åˆ›å»ºä¸€ä¸ªçˆ¬è™«å®ä¾‹
const myXCrawl = xCrawl({
  timeout: 10000, // è¯·æ±‚è¶…æ—¶æ—¶é—´
  intervalTime: { max: 3000, min: 2000 } // çˆ¬å–é—´éš”æ—¶é—´
})

// 3.è®¾ç½®çˆ¬å–ä»»åŠ¡
// è°ƒç”¨ startPolling API å¼€å§‹è½®è¯¢åŠŸèƒ½ï¼Œæ¯éš”ä¸€å¤©ä¼šè°ƒç”¨å›è°ƒå‡½æ•°
myXCrawl.startPolling({ d: 1 }, async (count, stopPolling) => {
  // è°ƒç”¨ crawlPage API çˆ¬å– Page
  const { page } = await myXCrawl.crawlPage('https://www.bilibili.com/guochuang/')

  // è®¾ç½®è¯·æ±‚é…ç½®ï¼Œè·å–è½®æ’­å›¾ç‰‡çš„ URL
  const requestConfig = await page.$$eval('.chief-recom-item img', (imgEls) =>
    imgEls.map((item) => item.src)
  )

  // è°ƒç”¨ crawlFile API çˆ¬å–å›¾ç‰‡
  myXCrawl.crawlFile({ requestConfig, fileConfig: { storeDir: './upload' } })

  // å…³é—­é¡µé¢
  page.close()
})
```

è¿è¡Œæ•ˆæœ:

<div align="center">
  <img src="https://raw.githubusercontent.com/coder-hxl/x-crawl/main/assets/cn/crawler.png" />
</div>

<div align="center">
  <img src="https://raw.githubusercontent.com/coder-hxl/x-crawl/main/assets/cn/crawler-result.png" />
</div>

**æ³¨æ„:** è¯·å‹¿éšæ„çˆ¬å–ï¼Œçˆ¬å–å‰å¯æŸ¥çœ‹ **robots.txt** åè®®ã€‚è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ x-crawl ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

### åˆ›å»ºåº”ç”¨

#### ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹

é€šè¿‡ [xCrawl()](#xCrawl) åˆ›å»ºä¸€ä¸ªæ–°çš„ **åº”ç”¨å®ä¾‹:**

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  // é€‰é¡¹
})
```

ç›¸å…³çš„ **é€‰é¡¹** å¯å‚è€ƒ [XCrawlBaseConfig](#XCrawlBaseConfig) ã€‚

#### é€‰æ‹©çˆ¬å–æ¨¡å¼

ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹æœ‰ä¸¤ç§çˆ¬å–æ¨¡å¼: å¼‚æ­¥/åŒæ­¥ï¼Œæ¯ä¸ªçˆ¬è™«å®ä¾‹åªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ç§ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  mode: 'async'
})
```

mode é€‰é¡¹é»˜è®¤ä¸º async ã€‚

- async: å¼‚æ­¥è¯·æ±‚ï¼Œåœ¨æ‰¹é‡è¯·æ±‚æ—¶ï¼Œæ— éœ€ç­‰å½“å‰è¯·æ±‚å®Œæˆï¼Œå°±è¿›è¡Œä¸‹æ¬¡è¯·æ±‚
- sync: åŒæ­¥è¯·æ±‚ï¼Œåœ¨æ‰¹é‡è¯·æ±‚æ—¶ï¼Œéœ€è¦ç­‰è¿™æ¬¡è¯·æ±‚å®Œæˆï¼Œæ‰ä¼šè¿›è¡Œä¸‹æ¬¡è¯·æ±‚

è‹¥æœ‰è®¾ç½®é—´éš”æ—¶é—´ï¼Œåˆ™éƒ½éœ€è¦ç­‰é—´éš”æ—¶é—´ç»“æŸæ‰èƒ½å‘é€è¯·æ±‚ã€‚

#### å¤šä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl1 = xCrawl({
  // é€‰é¡¹
})

const myXCrawl2 = xCrawl({
  // é€‰é¡¹
})
```

### çˆ¬å–é¡µé¢

é€šè¿‡ [crawlPage()](#crawlPage) çˆ¬å–ä¸€ä¸ªé¡µé¢ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({ timeout: 10000 })

myXCrawl.crawlPage('https://xxx.com').then((res) => {
  const { browser, page } = res

  // å…³é—­æµè§ˆå™¨
  browser.close()
})
```

#### browser å®ä¾‹

å®ƒæ˜¯ [Browser](https://pptr.dev/api/puppeteer.browser) çš„å®ä¾‹å¯¹è±¡ï¼Œå…·ä½“ä½¿ç”¨å¯ä»¥å‚è€ƒ [Browser](https://pptr.dev/api/puppeteer.browser) ã€‚

browser å®ä¾‹ä»–æ˜¯ä¸ªæ— å¤´æµè§ˆå™¨ï¼Œå¹¶æ—  UI å¤–å£³ï¼Œä»–åšçš„æ˜¯å°†æµè§ˆå™¨æ¸²æŸ“å¼•æ“æä¾›çš„**æ‰€æœ‰ç°ä»£ç½‘ç»œå¹³å°åŠŸèƒ½**å¸¦åˆ°ä»£ç ä¸­ã€‚

**æ³¨æ„ï¼š** browser å®ä¾‹å†…éƒ¨ä¼šä¸€ç›´äº§ç”Ÿäº‹ä»¶å¾ªç¯ï¼Œé€ æˆæ–‡ä»¶ä¸ä¼šç»ˆæ­¢ï¼Œå¦‚æœæƒ³åœæ­¢å¯ä»¥æ‰§è¡Œ browser.close() å…³é—­ã€‚å¦‚æœåé¢è¿˜éœ€è¦ç”¨åˆ° [crawlPage](#crawlPage) æˆ–è€… [page](#page) è¯·å‹¿è°ƒç”¨ã€‚å› ä¸ºå½“æ‚¨ä¿®æ”¹ browser å®ä¾‹çš„å±æ€§æ—¶ï¼Œä¼šå¯¹è¯¥çˆ¬è™«å®ä¾‹ crawlPage API å†…éƒ¨çš„ browser å®ä¾‹å’Œè¿”å›ç»“æœçš„ page å®ä¾‹ä»¥åŠ browser å®ä¾‹é€ æˆå½±å“ï¼Œå› ä¸º browser å®ä¾‹åœ¨åŒä¸€ä¸ªçˆ¬è™«å®ä¾‹çš„ crawlPage API å†…æ˜¯å…±äº«çš„ã€‚

#### page å®ä¾‹

å®ƒæ˜¯ [Page](https://pptr.dev/api/puppeteer.page) çš„å®ä¾‹å¯¹è±¡ï¼Œå®ä¾‹è¿˜å¯ä»¥åšäº‹ä»¶ä¹‹ç±»çš„äº¤äº’æ“ä½œï¼Œå…·ä½“ä½¿ç”¨å¯ä»¥å‚è€ƒ [page](https://pptr.dev/api/puppeteer.page) ã€‚

browser å®ä¾‹å†…éƒ¨ä¼šä¿ç•™ç€å¯¹ page å®ä¾‹çš„å¼•ç”¨ï¼Œå¦‚æœåç»­ä¸å†ä½¿ç”¨éœ€è¦è‡ªè¡Œå…³é—­ page å®ä¾‹ï¼Œå¦åˆ™ä¼šé€ æˆå†…å­˜æ³„éœ²ã€‚

**è‡ªè¡Œè§£æé¡µé¢**

ä»¥ä½¿ç”¨ jsdom åº“ä¸ºä¾‹ï¼š

```js
import xCrawl from 'x-crawl'
import { JSDOM } from 'jsdom'

const myXCrawl = xCrawl({ timeout: 10000 })

myXCrawl.crawlPage('https://www.xxx.com').then(async (res) => {
  const { page } = res

  // è·å–æœ€æ–°çš„é¡µé¢å†…å®¹
  const content = await page.content()

  // ä½¿ç”¨ jsdom åº“è‡ªè¡Œè§£æ
  const jsdom = new JSDOM(content)

  console.log(jsdom.window.document.querySelector('title').textContent)
})
```

**è·å–å±å¹•æˆªå›¾**

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({ timeout: 10000 })

myXCrawl.crawlPage('https://xxx.com').then(async (res) => {
  const { page } = res

  // è·å–é¡µé¢æ¸²æŸ“åçš„æˆªå›¾
  await page.screenshot({ path: './upload/page.png' })

  console.log('è·å–å±å¹•æˆªå›¾å®Œæ¯•')
})
```

### çˆ¬å–æ¥å£

é€šè¿‡ [crawlData()](#crawlData) çˆ¬å–æ¥å£æ•°æ®ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 3000, min: 1000 }
})

const requestConfig = [
  { url: 'https://xxx.com/xxxx' },
  { url: 'https://xxx.com/xxxx', method: 'POST', data: { name: 'coderhxl' } },
  { url: 'https://xxx.com/xxxx' }
]

myXCrawl.crawlData({ requestConfig }).then((res) => {
  // å¤„ç†
})
```

### çˆ¬å–æ–‡ä»¶

é€šè¿‡ [crawlFile()](#crawlFile) çˆ¬å–æ–‡ä»¶æ•°æ®ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 3000, min: 1000 }
})

const requestConfig = ['https://xxx.com/xxxx', 'https://xxx.com/xxxx']

myXCrawl
  .crawlFile({
    requestConfig,
    fileConfig: {
      storeDir: './upload' // å­˜æ”¾æ–‡ä»¶å¤¹
    }
  })
  .then((fileInfos) => {
    console.log(fileInfos)
  })
```

### å¯åŠ¨è½®è¯¢

é€šè¿‡ [startPolling()](#startPolling) å¯åŠ¨ä¸€ä¸ªè½®è¯¢çˆ¬å–ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000
})

myXCrawl.startPolling({ h: 2, m: 30 }, async (count, stopPolling) => {
  // æ¯éš”ä¸¤ä¸ªåŠå°æ—¶ä¼šæ‰§è¡Œä¸€æ¬¡
  // crawlPage/crawlData/crawlFile
  const { browser, page } = await myXCrawl.crawlPage('https://xxx.com')
  page.close()
})
```

**åœ¨è½®è¯¢ä¸­ä½¿ç”¨ crawlPage æ³¨æ„ï¼š** è°ƒç”¨ page.close() æ˜¯ä¸ºäº†é˜²æ­¢ browser å®ä¾‹å†…éƒ¨è¿˜ä¿ç•™ç€å¯¹ page å®ä¾‹çš„å¼•ç”¨ï¼Œå¦‚æœåç»­ä¸å†ä½¿ç”¨éœ€è¦è‡ªè¡Œå…³é—­ page å®ä¾‹ï¼Œå¦åˆ™ä¼šé€ æˆå†…å­˜æ³„éœ²ã€‚

å›è°ƒå‡½æ•°å‚æ•°ï¼š

- count å±æ€§è®°å½•å½“å‰æ˜¯ç¬¬å‡ æ¬¡è½®è¯¢æ“ä½œã€‚
- stopPolling æ˜¯ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œè°ƒç”¨å…¶å¯ä»¥ç»ˆæ­¢åé¢çš„è½®è¯¢æ“ä½œã€‚

### çˆ¬å–é—´éš”æ—¶é—´

è®¾ç½®çˆ¬å–é—´éš”æ—¶é—´å¯ä»¥é˜²æ­¢å¹¶å‘é‡å¤ªå¤§ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå¤ªå¤§çš„å‹åŠ›ã€‚

å¯ä»¥åœ¨åˆ›å»ºçˆ¬è™«å®ä¾‹çš„æ—¶å€™è®¾ç½®ï¼Œä¹Ÿå¯é€‰æ‹©ç»™æŸä¸ª API å•ç‹¬è®¾ç½®ã€‚çˆ¬å–é—´éš”æ—¶é—´æ˜¯ç”±å®ä¾‹æ–¹æ³•å†…éƒ¨æ§åˆ¶çš„ï¼Œå¹¶éç”±å®ä¾‹æ§åˆ¶æ•´ä¸ªçˆ¬å–é—´éš”æ—¶é—´ã€‚

```js
import xCrawl from 'x-crawl'

// ç»Ÿä¸€è®¾ç½®
const myXCrawl = xCrawl({
  intervalTime: { max: 3000, min: 1000 }
})

// å•ç‹¬è®¾ç½® (ä¼˜å…ˆçº§é«˜)
myXCrawl.crawlFile({
  requestConfig: ['https://xxx.com/xxxx', 'https://xxx.com/xxxx'],
  intervalTime: { max: 2000, min: 1000 }
})
```

intervalTime é€‰é¡¹é»˜è®¤ä¸º undefined ã€‚è‹¥æœ‰è®¾ç½®å€¼ï¼Œåˆ™ä¼šåœ¨è¯·æ±‚å‰ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¯ä»¥é˜²æ­¢å¹¶å‘é‡å¤ªå¤§ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå¤ªå¤§çš„å‹åŠ›ã€‚

- number: å›ºå®šæ¯æ¬¡è¯·æ±‚å‰å¿…é¡»ç­‰å¾…çš„æ—¶é—´
- Object: åœ¨ max å’Œ min ä¸­éšæœºå–ä¸€ä¸ªå€¼ï¼Œæ›´åŠ æ‹ŸäººåŒ–

**æ³¨æ„:** ç¬¬ä¸€æ¬¡è¯·æ±‚æ˜¯ä¸ä¼šè§¦å‘é—´éš”æ—¶é—´ã€‚

### requestConfig é€‰é¡¹çš„å¤šç§å†™æ³•

requestConfig çš„å†™æ³•éå¸¸çµæ´»ï¼Œä¸€å…±æœ‰ 5 ç§ï¼Œå¯ä»¥æ˜¯:

- å­—ç¬¦ä¸²
- å­—ç¬¦ä¸²æ•°ç»„
- å¯¹è±¡
- å¯¹è±¡æ•°ç»„
- å­—ç¬¦ä¸²åŠ å¯¹è±¡æ•°ç»„

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 3000, min: 1000 }
})

// requestConfig å†™æ³•1:
const requestConfig1 = 'https://xxx.com/xxxx'

// requestConfig å†™æ³•2:
const requestConfig2 = ['https://xxx.com/xxxx', 'https://xxx.com/xxxx', 'https://xxx.com/xxxx']

// requestConfig å†™æ³•3:
const requestConfig3 = {
  url: 'https://xxx.com/xxxx',
  method: 'POST',
  data: { name: 'coderhxl' }
}

// requestConfig å†™æ³•4:
const requestConfig4 = [
  { url: 'https://xxx.com/xxxx' },
  { url: 'https://xxx.com/xxxx', method: 'POST', data: { name: 'coderhxl' } },
  { url: 'https://xxx.com/xxxx' }
]

// requestConfig å†™æ³•5:
const requestConfig5 = [
  'https://xxx.com/xxxx',
  { url: 'https://xxx.com/xxxx', method: 'POST', data: { name: 'coderhxl' } },
  'https://xxx.com/xxxx'
]

myXCrawl.crawlData({ requestConfig: requestConfig5 }).then((res) => {
  console.log(res)
})
```

å¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰ç”¨å³å¯ã€‚

### è·å–ç»“æœçš„å¤šç§æ–¹å¼

è·å–ç»“æœæœ‰ä¸‰ç§æ–¹å¼: Promiseã€Callback ä»¥åŠ Promise + Callbackã€‚

- Promise: ç­‰æ‰€æœ‰è¯·æ±‚ç»“æŸåï¼Œè·å–æ‰€æœ‰è¯·æ±‚çš„ç»“æœ
- Callback: æ¯æ¬¡è¯·æ±‚ç»“æŸåï¼Œè·å–å½“å‰è¯·æ±‚çš„ç»“æœ

è¿™ä¸‰ç§æ–¹å¼é€‚ç”¨äº crawlPageã€crawlData ä»¥åŠ crawlFile ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 3000, min: 1000 }
})

const requestConfig = ['https://xxx.com/xxxx', 'https://xxx.com/xxxx', 'https://xxx.com/xxxx']

// æ–¹å¼ä¸€: Promise
myXCrawl
  .crawlFile({
    requestConfig,
    fileConfig: { storeDir: './upload' }
  })
  .then((fileInfos) => {
    console.log('Promise: ', fileInfos)
  })

// æ–¹å¼äºŒ: Callback
myXCrawl.crawlFile(
  {
    requestConfig,
    fileConfig: { storeDir: './upload' }
  },
  (fileInfo) => {
    console.log('Callback: ', fileInfo)
  }
)

// æ–¹å¼ä¸‰: Promise + Callback
myXCrawl
  .crawlFile(
    {
      requestConfig,
      fileConfig: { storeDir: './upload' }
    },
    (fileInfo) => {
      console.log('Callback: ', fileInfo)
    }
  )
  .then((fileInfos) => {
    console.log('Promise: ', fileInfos)
  })
```

å¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰ç”¨å³å¯ã€‚

## API

### xCrawl

é€šè¿‡è°ƒç”¨ xCrawl åˆ›å»ºä¸€ä¸ªçˆ¬è™«å®ä¾‹ã€‚è¯·æ±‚æ˜¯ç”±å®ä¾‹æ–¹æ³•å†…éƒ¨è‡ªå·±ç»´æŠ¤ï¼Œå¹¶éç”±å®ä¾‹è‡ªå·±ç»´æŠ¤ã€‚

#### ç±»å‹

- [XCrawlBaseConfig](#XCrawlBaseConfig)
- [XCrawlInstance](#XCrawlInstance)

```ts
function xCrawl(baseConfig?: XCrawlBaseConfig): XCrawlInstance
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

// xCrawl API
const myXCrawl = xCrawl({
  baseUrl: 'https://xxx.com',
  timeout: 10000,
  // çˆ¬å–é—´éš”æ—¶é—´, æ‰¹é‡çˆ¬å–æ‰æœ‰æ•ˆ
  intervalTime: {
    max: 2000,
    min: 1000
  }
})
```

### crawlPage

crawlPage æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºçˆ¬å–é¡µé¢ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [CrawlPageConfig](#CrawlPageConfig) ç±»å‹
- æŸ¥çœ‹ [CrawlPage](#CrawlPage-1) ç±»å‹

```ts
function crawlPage<T extends CrawlPageConfig = CrawlPageConfig>(
  config: T,
  callback?: ((res: CrawlPage) => void) | undefined
): Promise<T extends string[] | CrawlBaseConfigV1[] ? CrawlPage[] : CrawlPage>
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({ timeout: 10000 })

// crawlPage API
myXCrawl.crawlPage('https://xxx.com/xxx').then((res) => {
  const { browser, page } = res

  // å…³é—­æµè§ˆå™¨
  browser.close()
})
```

### crawlData

crawl æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºçˆ¬å– API ï¼Œå¯è·å– JSON æ•°æ®ç­‰ç­‰ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [CrawlDataConfig](#CrawlDataConfig) ç±»å‹
- æŸ¥çœ‹ [CrawlResCommonV1](#CrawlResCommonV1) ç±»å‹
- æŸ¥çœ‹ [CrawlResCommonArrV1](#CrawlResCommonArrV1) ç±»å‹

```ts
function crawlData: <T = any>(
  config: CrawlDataConfig,
  callback?: (res: CrawlResCommonV1<T>) => void
) => Promise<CrawlResCommonArrV1<T>>
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})

const requestConfig = [
  { url: 'https://xxx.com/xxxx' },
  { url: 'https://xxx.com/xxxx', method: 'POST', data: { name: 'coderhxl' } },
  { url: 'https://xxx.com/xxxx' }
]

// crawlData API
myXCrawl.crawlData({ requestConfig }).then((res) => {
  console.log(res)
})
```

### crawlFile

crawlFile æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºçˆ¬å–æ–‡ä»¶ï¼Œå¯è·å–å›¾ç‰‡ã€pdf æ–‡ä»¶ç­‰ç­‰ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [CrawlFileConfig](#CrawlFileConfig) ç±»å‹
- æŸ¥çœ‹ [CrawlResCommonV1](#CrawlResCommonV1) ç±»å‹
- æŸ¥çœ‹ [CrawlResCommonArrV1](#CrawlResCommonArrV1) ç±»å‹
- æŸ¥çœ‹ [FileInfo](#FileInfo) ç±»å‹

```ts
function crawlFile: (
  config: CrawlFileConfig,
  callback?: (res: CrawlResCommonV1<FileInfo>) => void
) => Promise<CrawlResCommonArrV1<FileInfo>>
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})

const requestConfig = ['https://xxx.com/xxxx', 'https://xxx.com/xxxx']

// crawlFile API
myXCrawl
  .crawlFile({
    requestConfig,
    fileConfig: {
      storeDir: './upload' // å­˜æ”¾æ–‡ä»¶å¤¹
    }
  })
  .then((fileInfos) => {
    console.log(fileInfos)
  })
```

### startPolling

crawlPolling æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºè¿›è¡Œè½®è¯¢æ“ä½œï¼Œæ¯”å¦‚æ¯éš”ä¸€æ®µæ—¶é—´è·å–æ–°é—»ä¹‹ç±»çš„ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [StartPollingConfig](#StartPollingConfig) ç±»å‹

```ts
function startPolling: (
  config: StartPollingConfig,
  callback: (count: number, stopPolling: () => void) => void
) => void
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})

// startPolling API
myXCrawl.startPolling({ h: 2, m: 30 }, (count, stopPolling) => {
  // æ¯éš”ä¸¤ä¸ªåŠå°æ—¶ä¼šæ‰§è¡Œä¸€æ¬¡
  // crawlPage/crawlData/crawlFile
})
```

## ç±»å‹

### AnyObject

```ts
interface AnyObject extends Object {
  [key: string | number | symbol]: any
}
```

### Method

```ts
type Method =
  | 'get'
  | 'GET'
  | 'delete'
  | 'DELETE'
  | 'head'
  | 'HEAD'
  | 'options'
  | 'OPTONS'
  | 'post'
  | 'POST'
  | 'put'
  | 'PUT'
  | 'patch'
  | 'PATCH'
  | 'purge'
  | 'PURGE'
  | 'link'
  | 'LINK'
  | 'unlink'
  | 'UNLINK'
```

### RequestConfigObjectV1

```ts
interface RequestConfigObjectV1 {
  url: string
  headers?: AnyObject
  timeout?: number
  proxy?: string
}
```

### RequestConfigObjectV2

```ts
interface RequestConfigObjectV2 {
  url: string
  method?: Method
  headers?: AnyObject
  params?: AnyObject
  data?: any
  timeout?: number
  proxy?: string
}
```

### RequestConfig

```ts
type RequestConfig = string | RequestConfigObjectV2
```

### IntervalTime

```ts
type IntervalTime =
  | number
  | {
      max: number
      min?: number
    }
```

### XCrawlBaseConfig

```ts
interface XCrawlBaseConfig {
  baseUrl?: string
  timeout?: number
  intervalTime?: IntervalTime
  mode?: 'async' | 'sync'
  proxy?: string
}
```

### CrawlBaseConfigV1

```ts
interface CrawlBaseConfigV1 extends RequestConfigObjectV1 {
  cookies?: string | Protocol.Network.CookieParam | Protocol.Network.CookieParam[] // Protocol æ¥è‡ª puppeteer åº“
}
```

### CrawlBaseConfigV2

```ts
interface CrawlBaseConfigV2 {
  requestConfig: RequestConfig | RequestConfig[]
  intervalTime?: IntervalTime
}
```

### CrawlPageConfig

```ts
type CrawlPageConfig = string | string[] | CrawlBaseConfigV1 | CrawlBaseConfigV1[]
```

### CrawlDataConfig

```ts
interface CrawlDataConfig extends CrawlBaseConfigV2 {}
```

### CrawlFileConfig

```ts
interface CrawlFileConfig extends CrawlBaseConfigV2 {
  fileConfig: {
    storeDir: string // å­˜æ”¾æ–‡ä»¶å¤¹
    extension?: string // æ–‡ä»¶æ‰©å±•å
  }
}
```

### StartPollingConfig

```ts
interface StartPollingConfig {
  d?: number // æ—¥
  h?: number // å°æ—¶
  m?: number // åˆ†é’Ÿ
}
```

### XCrawlInstance

```js
interface XCrawlInstance {
  crawlPage: <T extends CrawlPageConfig = CrawlPageConfig>(
    config: T,
    callback?: (res: CrawlPage) => void
  ) => Promise<
    T extends string[] | CrawlBaseConfigV1[] ? CrawlPage[] : CrawlPage
  >

  crawlData: <T = any>(
    config: CrawlDataConfig,
    callback?: (res: CrawlResCommonV1<T>) => void
  ) => Promise<CrawlResCommonArrV1<T>>

  crawlFile: (
    config: CrawlFileConfig,
    callback?: (res: CrawlResCommonV1<FileInfo>) => void
  ) => Promise<CrawlResCommonArrV1<FileInfo>>

  startPolling: (
    config: StartPollingConfig,
    callback: (count: number, stopPolling: () => void) => void
  ) => void
}
```

### CrawlResCommonV1

```ts
interface CrawlResCommonV1<T> {
  id: number
  statusCode: number | undefined
  headers: IncomingHttpHeaders // nodejs: http ç±»å‹
  data: T
}
```

### CrawlResCommonArrV1

```ts
type CrawlResCommonArrV1<T> = CrawlResCommonV1<T>[]
```

### CrawlPage

```ts
interface CrawlPage {
  httpResponse: HTTPResponse | null // puppeteer åº“çš„ HTTPResponse ç±»å‹
  browser: Browser // puppeteer åº“çš„ Browser ç±»å‹
  page: Page // puppeteer åº“çš„ Page ç±»å‹
}
```

### FileInfo

```ts
interface FileInfo {
  fileName: string
  mimeType: string
  size: number
  filePath: string
}
```

## æ›´å¤š

å¦‚æœ‰ **é—®é¢˜** æˆ– **éœ€æ±‚** è¯·åœ¨ https://github.com/coder-hxl/x-crawl/issues ä¸­æ **Issues** ã€‚
