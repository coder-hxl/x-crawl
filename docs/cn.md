# x-crawl [![npm](https://img.shields.io/npm/v/x-crawl.svg)](https://www.npmjs.com/package/x-crawl) [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/coder-hxl/x-crawl/blob/main/LICENSE)

[English](https://github.com/coder-hxl/x-crawl#x-crawl) | ç®€ä½“ä¸­æ–‡

x-crawl æ˜¯ä¸€ä¸ªçµæ´»çš„ nodejs çˆ¬è™«åº“ã€‚å¯æ‰¹é‡çˆ¬å–é¡µé¢ã€æ‰¹é‡ç½‘ç»œè¯·æ±‚ã€æ‰¹é‡ä¸‹è½½æ–‡ä»¶èµ„æºã€è½®è¯¢çˆ¬å–ç­‰ã€‚ç”¨æ³•çµæ´»å’Œç®€å•ï¼Œå¯¹ JS/TS å¼€å‘è€…å‹å¥½ã€‚

> å¦‚æœä½ å–œæ¬¢ x-crawl ï¼Œå¯ä»¥ç»™ [x-crawl å­˜å‚¨åº“](https://github.com/coder-hxl/x-crawl) ç‚¹ä¸ª Star æ”¯æŒä¸€ä¸‹ï¼Œä¸ä»…æ˜¯å¯¹å®ƒçš„è®¤å¯ï¼ŒåŒæ—¶ä¹Ÿæ˜¯å¯¹å¼€å‘è€…çš„è®¤å¯ã€‚

## ç‰¹å¾

- **ğŸ”¥ å¼‚æ­¥/åŒæ­¥** - åªéœ€æ›´æ”¹ä¸€ä¸‹ mode å±æ€§å³å¯åˆ‡æ¢ å¼‚æ­¥/åŒæ­¥ çˆ¬å–æ¨¡å¼ã€‚
- **âš™ï¸ å¤šç§åŠŸèƒ½** - å¯æ‰¹é‡çˆ¬å–é¡µé¢ã€æ‰¹é‡ç½‘ç»œè¯·æ±‚ã€æ‰¹é‡ä¸‹è½½æ–‡ä»¶èµ„æºã€è½®è¯¢çˆ¬å–ç­‰ã€‚
- **ğŸ–‹ï¸ å†™æ³•çµæ´»** - ä¸€ç§åŠŸèƒ½é€‚é…å¤šç§çˆ¬å–é…ç½®ã€è·å–çˆ¬å–ç»“æœçš„å†™æ³•ï¼Œå†™æ³•éå¸¸çµæ´»ã€‚
- **â±ï¸ é—´éš”çˆ¬å–** - æ— é—´éš”/å›ºå®šé—´éš”/éšæœºé—´éš”ï¼Œå¯ä»¥æœ‰æ•ˆ ä½¿ç”¨/é¿å… é«˜å¹¶å‘çˆ¬å–ã€‚
- **ğŸ”„ å¤±è´¥é‡è¯•** - å¯é’ˆå¯¹æ‰€æœ‰çˆ¬å–çš„è¯·æ±‚è®¾ç½®ï¼Œé’ˆå¯¹å•æ¬¡çˆ¬å–çš„è¯·æ±‚è®¾ç½®ï¼Œé’ˆå¯¹å•ä¸ªè¯·æ±‚è®¾ç½®è¿›è¡Œå¤±è´¥é‡è¯•ã€‚
- **ğŸš€ ä¼˜å…ˆé˜Ÿåˆ—** - æ ¹æ®å•ä¸ªè¯·æ±‚çš„ä¼˜å…ˆçº§ä½¿ç”¨ä¼˜å…ˆçˆ¬å–ã€‚
- **â˜ï¸ çˆ¬å– SPA** - æ‰¹é‡çˆ¬å– SPAï¼ˆå•é¡µåº”ç”¨ç¨‹åºï¼‰ç”Ÿæˆé¢„æ¸²æŸ“å†…å®¹ï¼ˆå³â€œSSRâ€ï¼ˆæœåŠ¡å™¨ç«¯æ¸²æŸ“ï¼‰ï¼‰ã€‚
- **âš’ï¸ æ§åˆ¶é¡µé¢** - æ— å¤´æµè§ˆå™¨å¯ä»¥è¡¨å•æäº¤ã€é”®ç›˜è¾“å…¥ã€äº‹ä»¶æ“ä½œã€ç”Ÿæˆé¡µé¢çš„å±å¹•æˆªå›¾ç­‰ã€‚
- **ğŸ§¾ æ•è·è®°å½•** - å¯¹çˆ¬å–çš„ç»“æœè¿›è¡Œæ•è·è®°å½•ï¼Œå¹¶åœ¨æ§åˆ¶å°è¿›è¡Œé«˜äº®çš„æé†’ã€‚
- **ğŸ¦¾TypeScript** - æ‹¥æœ‰ç±»å‹ï¼Œé€šè¿‡æ³›å‹å®ç°å®Œæ•´çš„ç±»å‹ã€‚

## è·Ÿ puppeteer çš„å…³ç³»

crawlPage API å†…éƒ¨ä½¿ç”¨ [puppeteer](https://github.com/puppeteer/puppeteer) åº“æ¥å¸®åŠ©æˆ‘ä»¬çˆ¬å–é¡µé¢ï¼Œå¹¶å°† Brower å®ä¾‹å’Œ Page å®ä¾‹æš´éœ²å‡ºæ¥ã€‚

# ç›®å½•

- [å®‰è£…](#å®‰è£…)
- [ç¤ºä¾‹](#ç¤ºä¾‹)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
  - [åˆ›å»ºåº”ç”¨](#åˆ›å»ºåº”ç”¨)
    - [ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹](#ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹)
    - [é€‰æ‹©çˆ¬å–æ¨¡å¼](#é€‰æ‹©çˆ¬å–æ¨¡å¼)
    - [å¤šä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹](#å¤šä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹)
  - [çˆ¬å–é¡µé¢](#çˆ¬å–é¡µé¢)
    - [browser å®ä¾‹](#browser-å®ä¾‹)
    - [page å®ä¾‹](#page-å®ä¾‹)
  - [çˆ¬å–æ¥å£](#çˆ¬å–æ¥å£)
  - [çˆ¬å–æ–‡ä»¶](#çˆ¬å–æ–‡ä»¶)
  - [å¯åŠ¨è½®è¯¢](#å¯åŠ¨è½®è¯¢)
  - [é…ç½®ä¼˜å…ˆçº§](#é…ç½®ä¼˜å…ˆçº§)
  - [é—´éš”æ—¶é—´](#é—´éš”æ—¶é—´)
  - [å¤±è´¥é‡è¯•](#å¤±è´¥é‡è¯•)
  - [ä¼˜å…ˆé˜Ÿåˆ—](#ä¼˜å…ˆé˜Ÿåˆ—)
  - [å…³äºç»“æœ](#å…³äºç»“æœ)
  - [TypeScript](#TypeScript)
- [API](#API)
  - [xCrawl](#xCrawl)
    - [ç±»å‹](#ç±»å‹)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-1)
  - [crawlPage](#crawlPage)
    - [ç±»å‹](#ç±»å‹-1)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-2)
    - [é…ç½®](#é…ç½®)
  - [crawlData](#crawlData)
    - [ç±»å‹](#ç±»å‹-2)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-3)
    - [é…ç½®](#é…ç½®-1)
  - [crawlFile](#crawlFile)
    - [ç±»å‹](#ç±»å‹-3)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-4)
    - [é…ç½®](#é…ç½®-2)
  - [startPolling](#startPolling)
    - [ç±»å‹](#ç±»å‹-4)
    - [ç¤ºä¾‹](#ç¤ºä¾‹-5)
    - [ç±»å‹](#ç±»å‹-5)
- [ç±»å‹](#ç±»å‹-6)
  - [API Config](#API-Config)
    - [API Config Other](#API-Config-Other)
      - [IntervalTime](#IntervalTime)
      - [Method](#Method)
      - [PageRequestConfigCookies](#PageRequestConfigCookies)
    - [API Config Request](#API-Config-Request)
      - [PageRequestConfig](#PageRequestConfig)
      - [DataRequestConfig](#DataRequestConfig)
      - [FileRequestConfig](#FileRequestConfig)
    - [API Config Crawl](#API-Config-Crawl)
      - [XCrawlBaseConfig](#XCrawlBaseConfig)
      - [CrawlPageConfigObject](#CrawlPageConfigObject)
      - [CrawlDataConfigObject](#CrawlDataConfigObject)
      - [CrawlFileConfigObject](#CrawlFileConfigObject)
      - [CrawlPageConfig](#CrawlPageConfig)
      - [CrawlDataConfig](#CrawlDataConfig)
      - [CrawlFileConfig](#CrawlFileConfig)
      - [StartPollingConfig](#StartPollingConfig)
  - [API Result](#API-Result)
    - [XCrawlInstance](#XCrawlInstance)
    - [CrawlCommonRes](#CrawlCommonRes)
    - [CrawlPageSingleRes](#CrawlPageSingleRes)
    - [CrawlDataSingleRes](#CrawlDataSingleRes)
    - [CrawlFileSingleRes](#CrawlFileSingleRes)
    - [CrawlPageRes](#CrawlPageRes)
    - [CrawlDataRes](#CrawlDataRes)
    - [CrawlFileRes](#CrawlFileRes)
  - [API Other](#API-Other)
    - [AnyObject](#AnyObject)
- [æ›´å¤š](#æ›´å¤š)

## å®‰è£…

ä»¥ NPM ä¸ºä¾‹:

```shell
npm install x-crawl
```

## ç¤ºä¾‹

æ¯å¤©è‡ªåŠ¨è·å– bilibili é¦–é¡µã€å›½æ¼«ã€ç”µå½±è¿™ä¸‰ä¸ªé¡µé¢çš„è½®æ’­å›¾ç‰‡ä¸ºä¾‹:

```js
// 1.å¯¼å…¥æ¨¡å— ES/CJS
import xCrawl from 'x-crawl'

// 2.åˆ›å»ºä¸€ä¸ªçˆ¬è™«å®ä¾‹
const myXCrawl = xCrawl({ maxRetry: 3, intervalTime: { max: 3000, min: 2000 } })

// 3.è®¾ç½®çˆ¬å–ä»»åŠ¡
// è°ƒç”¨ startPolling API å¼€å§‹è½®è¯¢åŠŸèƒ½ï¼Œæ¯éš”ä¸€å¤©ä¼šè°ƒç”¨å›è°ƒå‡½æ•°
myXCrawl.startPolling({ d: 1 }, async (count, stopPolling) => {
  // è°ƒç”¨ crawlPage API çˆ¬å– é¦–é¡µã€å›½æ¼«ã€ç”µå½± è¿™ä¸‰ä¸ªé¡µé¢
  const res = await myXCrawl.crawlPage([
    'https://www.bilibili.com',
    'https://www.bilibili.com/guochuang',
    'https://www.bilibili.com/movie'
  ])

  // å­˜æ”¾å›¾ç‰‡ URL
  const imgUrls: string[] = []
  const elSelectorMap = ['.carousel-inner img', '.chief-recom-item img', '.bg-item img']
  for (const item of res) {
    const { id } = item
    const { page } = item.data

    // è·å–é¡µé¢è½®æ’­å›¾ç‰‡å…ƒç´ çš„ URL
    const urls = await page.$$eval(elSelectorMap[id - 1], (imgEls) =>
      imgEls.map((item) => item.src)
    )
    imgUrls.push(...urls)

    // å…³é—­é¡µé¢
    page.close()
  }

  // è°ƒç”¨ crawlFile API çˆ¬å–å›¾ç‰‡
  await myXCrawl.crawlFile({
    requestConfigs: imgUrls,
    fileConfig: { storeDir: './upload' }
  })
})
```

è¿è¡Œæ•ˆæœ:

<div align="center">
  <img src="https://raw.githubusercontent.com/coder-hxl/x-crawl/main/assets/cn/crawler.png" />
</div>

<div align="center">
  <img src="https://raw.githubusercontent.com/coder-hxl/x-crawl/main/assets/cn/crawler-result.png" />
</div>

**æ³¨æ„:** è¯·å‹¿éšæ„çˆ¬å–ï¼Œçˆ¬å–å‰å¯æŸ¥çœ‹ **robots.txt** åè®®ã€‚è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ x-crawl ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

### åˆ›å»ºåº”ç”¨

#### ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹

é€šè¿‡ [xCrawl()](#xCrawl) åˆ›å»ºä¸€ä¸ªæ–°çš„ **åº”ç”¨å®ä¾‹:**

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  // é€‰é¡¹
})
```

ç›¸å…³çš„ **é€‰é¡¹** å¯å‚è€ƒ [XCrawlBaseConfig](#XCrawlBaseConfig) ã€‚

#### é€‰æ‹©çˆ¬å–æ¨¡å¼

ä¸€ä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹æœ‰ä¸¤ç§çˆ¬å–æ¨¡å¼: å¼‚æ­¥/åŒæ­¥ï¼Œæ¯ä¸ªçˆ¬è™«å®ä¾‹åªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ç§ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  mode: 'async'
})
```

mode é€‰é¡¹é»˜è®¤ä¸º async ã€‚

- async: å¼‚æ­¥è¯·æ±‚ï¼Œåœ¨æ‰¹é‡è¯·æ±‚æ—¶ï¼Œæ— éœ€ç­‰å½“å‰è¯·æ±‚å®Œæˆï¼Œå°±è¿›è¡Œä¸‹æ¬¡è¯·æ±‚
- sync: åŒæ­¥è¯·æ±‚ï¼Œåœ¨æ‰¹é‡è¯·æ±‚æ—¶ï¼Œéœ€è¦ç­‰è¿™æ¬¡è¯·æ±‚å®Œæˆï¼Œæ‰ä¼šè¿›è¡Œä¸‹æ¬¡è¯·æ±‚

è‹¥æœ‰è®¾ç½®é—´éš”æ—¶é—´ï¼Œåˆ™éƒ½éœ€è¦ç­‰é—´éš”æ—¶é—´ç»“æŸæ‰èƒ½å‘é€è¯·æ±‚ã€‚

#### å¤šä¸ªçˆ¬è™«åº”ç”¨å®ä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl1 = xCrawl({
  // é€‰é¡¹
})

const myXCrawl2 = xCrawl({
  // é€‰é¡¹
})
```

### çˆ¬å–é¡µé¢

é€šè¿‡ [crawlPage()](#crawlPage) çˆ¬å–ä¸€ä¸ªé¡µé¢ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlPage('https://xxx.com').then((res) => {
  const { browser, page } = res.data

  // å…³é—­æµè§ˆå™¨
  browser.close()
})
```

#### browser å®ä¾‹

å®ƒæ˜¯ [Browser](https://pptr.dev/api/puppeteer.browser) çš„å®ä¾‹å¯¹è±¡ï¼Œå…·ä½“ä½¿ç”¨å¯ä»¥å‚è€ƒ [Browser](https://pptr.dev/api/puppeteer.browser) ã€‚

browser å®ä¾‹ä»–æ˜¯ä¸ªæ— å¤´æµè§ˆå™¨ï¼Œå¹¶æ—  UI å¤–å£³ï¼Œä»–åšçš„æ˜¯å°†æµè§ˆå™¨æ¸²æŸ“å¼•æ“æä¾›çš„**æ‰€æœ‰ç°ä»£ç½‘ç»œå¹³å°åŠŸèƒ½**å¸¦åˆ°ä»£ç ä¸­ã€‚

**æ³¨æ„ï¼š** browser å®ä¾‹å†…éƒ¨ä¼šä¸€ç›´äº§ç”Ÿäº‹ä»¶å¾ªç¯ï¼Œé€ æˆæ–‡ä»¶ä¸ä¼šç»ˆæ­¢ï¼Œå¦‚æœæƒ³åœæ­¢å¯ä»¥æ‰§è¡Œ browser.close() å…³é—­ã€‚å¦‚æœåé¢è¿˜éœ€è¦ç”¨åˆ° [crawlPage](#crawlPage) æˆ–è€… [page](#page) è¯·å‹¿è°ƒç”¨ã€‚å› ä¸ºå½“æ‚¨ä¿®æ”¹ browser å®ä¾‹çš„å±æ€§æ—¶ï¼Œä¼šå¯¹è¯¥çˆ¬è™«å®ä¾‹ crawlPage API å†…éƒ¨çš„ browser å®ä¾‹å’Œè¿”å›ç»“æœçš„ page å®ä¾‹ä»¥åŠ browser å®ä¾‹é€ æˆå½±å“ï¼Œå› ä¸º browser å®ä¾‹åœ¨åŒä¸€ä¸ªçˆ¬è™«å®ä¾‹çš„ crawlPage API å†…æ˜¯å…±äº«çš„ã€‚

#### page å®ä¾‹

å®ƒæ˜¯ [Page](https://pptr.dev/api/puppeteer.page) çš„å®ä¾‹å¯¹è±¡ï¼Œå®ä¾‹è¿˜å¯ä»¥åšäº‹ä»¶ä¹‹ç±»çš„äº¤äº’æ“ä½œï¼Œå…·ä½“ä½¿ç”¨å¯ä»¥å‚è€ƒ [page](https://pptr.dev/api/puppeteer.page) ã€‚

browser å®ä¾‹å†…éƒ¨ä¼šä¿ç•™ç€å¯¹ page å®ä¾‹çš„å¼•ç”¨ï¼Œå¦‚æœåç»­ä¸å†ä½¿ç”¨éœ€è¦è‡ªè¡Œå…³é—­ page å®ä¾‹ï¼Œå¦åˆ™ä¼šé€ æˆå†…å­˜æ³„éœ²ã€‚

**è·å–å±å¹•æˆªå›¾**

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlPage('https://xxx.com').then(async (res) => {
  const { browser, page } = res.data

  // è·å–é¡µé¢æ¸²æŸ“åçš„æˆªå›¾
  await page.screenshot({ path: './upload/page.png' })

  console.log('è·å–å±å¹•æˆªå›¾å®Œæ¯•')

  browser.close()
})
```

### çˆ¬å–æ¥å£

é€šè¿‡ [crawlData()](#crawlData) çˆ¬å–æ¥å£æ•°æ®ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({ intervalTime: { max: 3000, min: 1000 } })

const requestConfigs = [
  'https://xxx.com/xxxx',
  'https://xxx.com/xxxx',
  { url: 'https://xxx.com/xxxx', method: 'POST', data: { name: 'coderhxl' } }
]

myXCrawl.crawlData({ requestConfigs }).then((res) => {
  // å¤„ç†
})
```

### çˆ¬å–æ–‡ä»¶

é€šè¿‡ [crawlFile()](#crawlFile) çˆ¬å–æ–‡ä»¶æ•°æ®ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({ intervalTime: { max: 3000, min: 1000 } })

myXCrawl
  .crawlFile({
    requestConfigs: ['https://xxx.com/xxxx', 'https://xxx.com/xxxx'],
    fileConfig: {
      storeDir: './upload' // å­˜æ”¾æ–‡ä»¶å¤¹
    }
  })
  .then((res) => {})
```

### å¯åŠ¨è½®è¯¢

é€šè¿‡ [startPolling()](#startPolling) å¯åŠ¨ä¸€ä¸ªè½®è¯¢çˆ¬å–ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000
})

myXCrawl.startPolling({ h: 2, m: 30 }, async (count, stopPolling) => {
  // æ¯éš”ä¸¤ä¸ªåŠå°æ—¶ä¼šæ‰§è¡Œä¸€æ¬¡
  // crawlPage/crawlData/crawlFile
  const res = await myXCrawl.crawlPage('https://xxx.com')
  res.data.page.close()
})
```

**åœ¨è½®è¯¢ä¸­ä½¿ç”¨ crawlPage æ³¨æ„ï¼š** è°ƒç”¨ page.close() æ˜¯ä¸ºäº†é˜²æ­¢ browser å®ä¾‹å†…éƒ¨è¿˜ä¿ç•™ç€å¯¹ page å®ä¾‹çš„å¼•ç”¨ï¼Œå¦‚æœåç»­ä¸å†ä½¿ç”¨å½“å‰ page éœ€è¦è‡ªè¡Œå…³é—­ï¼Œå¦åˆ™ä¼šé€ æˆå†…å­˜æ³„éœ²ã€‚

å›è°ƒå‡½æ•°å‚æ•°ï¼š

- count å±æ€§è®°å½•å½“å‰æ˜¯ç¬¬å‡ æ¬¡è½®è¯¢æ“ä½œã€‚
- stopPolling æ˜¯ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œè°ƒç”¨å…¶å¯ä»¥ç»ˆæ­¢åé¢çš„è½®è¯¢æ“ä½œã€‚

### é…ç½®ä¼˜å…ˆçº§

ä¸€äº›é€šç”¨çš„é…ç½®å¯ä»¥åœ¨ä¸‰ä¸ªåœ°æ–¹è®¾ç½®ï¼š

- çˆ¬è™«åº”ç”¨å®ä¾‹ï¼ˆå…¨å±€ï¼‰
- çˆ¬è™« API ï¼ˆå±€éƒ¨ï¼‰
- è¯·æ±‚é…ç½®ï¼ˆå•ç‹¬ï¼‰

ä¼˜å…ˆçº§ä¸ºï¼šrequest config > API config > application config

### é—´éš”æ—¶é—´

é—´éš”æ—¶é—´å¯ä»¥é˜²æ­¢å¹¶å‘é‡å¤ªå¤§ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå¤ªå¤§çš„å‹åŠ›ã€‚

çˆ¬å–é—´éš”æ—¶é—´æ˜¯ç”±å®ä¾‹æ–¹æ³•å†…éƒ¨æ§åˆ¶çš„ï¼Œå¹¶éç”±å®ä¾‹æ§åˆ¶æ•´ä¸ªçˆ¬å–é—´éš”æ—¶é—´ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlData({
    requestConfigs: ['https://xxx.com/xxxx', 'https://xxx.com/xxxx'],
    intervalTime: { max: 2000, min: 1000 }
  })
  .then((res) => {})
```

intervalTime é€‰é¡¹é»˜è®¤ä¸º undefined ã€‚è‹¥æœ‰è®¾ç½®å€¼ï¼Œåˆ™ä¼šåœ¨è¯·æ±‚å‰ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¯ä»¥é˜²æ­¢å¹¶å‘é‡å¤ªå¤§ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå¤ªå¤§çš„å‹åŠ›ã€‚

- number: å›ºå®šæ¯æ¬¡è¯·æ±‚å‰å¿…é¡»ç­‰å¾…çš„æ—¶é—´
- Object: åœ¨ max å’Œ min ä¸­éšæœºå–ä¸€ä¸ªå€¼ï¼Œæ›´åŠ æ‹ŸäººåŒ–

**æ³¨æ„:** ç¬¬ä¸€æ¬¡è¯·æ±‚æ˜¯ä¸ä¼šè§¦å‘é—´éš”æ—¶é—´ã€‚

### å¤±è´¥é‡è¯•

å¤±è´¥é‡è¯•åœ¨è¶…æ—¶ä¹‹ç±»çš„é”™è¯¯å‘ç”Ÿæ—¶ï¼Œå°†ä¼šç­‰å¾…è¿™ä¸€è½®è¯·æ±‚ç»“æŸåé‡æ–°è¯·æ±‚ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlData({ url: 'https://xxx.com/xxxx', maxRetry: 1 }).then((res) => {})
```

maxRetry å±æ€§å†³å®šè¦é‡è¯•å‡ æ¬¡ã€‚

### ä¼˜å…ˆé˜Ÿåˆ—

ä¼˜å…ˆé˜Ÿåˆ—å¯ä»¥è®©æŸä¸ªè¯·æ±‚ä¼˜å…ˆå‘é€ã€‚

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlData([
    { url: 'https://xxx.com/xxxx', priority: 1 },
    { url: 'https://xxx.com/xxxx', priority: 10 },
    { url: 'https://xxx.com/xxxx', priority: 8 }
  ])
  .then((res) => {})
```

priority å±æ€§çš„å€¼è¶Šå¤§å°±åœ¨å½“å‰çˆ¬å–é˜Ÿåˆ—ä¸­è¶Šä¼˜å…ˆã€‚

### å…³äºç»“æœ

å¯¹äºç»“æœï¼Œæ¯ä¸ªè¯·æ±‚çš„ç»“æœå°†ç»Ÿä¸€ä½¿ç”¨å¯¹è±¡åŒ…è£¹ç€ï¼Œè¯¥å¯¹è±¡æä¾›äº†å…³äºè¿™æ¬¡è¯·æ±‚ç»“æœçš„ä¿¡æ¯ï¼Œæ¯”å¦‚ï¼šidã€ç»“æœã€æ˜¯å¦æˆåŠŸã€æœ€å¤§é‡è¯•ã€é‡è¯•æ¬¡æ•°ã€æ”¶é›†åˆ°é”™è¯¯ä¿¡æ¯ç­‰ã€‚è‡ªåŠ¨æ ¹æ®ä½ é€‰ç”¨çš„é…ç½®æ–¹å¼å†³å®šè¿”å›å€¼æ˜¯å¦åŒ…è£¹åœ¨ä¸€ä¸ªæ•°ç»„ä¸­ï¼Œå¹¶ä¸”åœ¨ TS ä¸­ç±»å‹å®Œç¾é€‚é…ã€‚

æ¯ä¸ªå¯¹è±¡çš„ id æ˜¯æ ¹æ®ä½ é…ç½®é‡Œçš„è¯·æ±‚é¡ºåºå†³å®šçš„ï¼Œå¦‚æœæœ‰ä½¿ç”¨ä¼˜å…ˆçº§ï¼Œåˆ™ä¼šæ ¹æ®ä¼˜å…ˆçº§æ’åºã€‚

ç›¸å…³çš„é…ç½®æ–¹å¼å’Œç»“æœè¯¦æƒ…æŸ¥çœ‹ï¼š[crawlPage é…ç½®](#é…ç½®)ã€[crawlData é…ç½®](#é…ç½®-1)ã€[crawlFile é…ç½®](#é…ç½®-2) ã€‚

### TypeScript

åƒ TypeScript è¿™æ ·çš„ç±»å‹ç³»ç»Ÿå¯ä»¥åœ¨ç¼–è¯‘æ—¶é€šè¿‡é™æ€åˆ†ææ£€æµ‹å‡ºå¾ˆå¤šå¸¸è§é”™è¯¯ã€‚è¿™å‡å°‘äº†è¿è¡Œæ—¶é”™è¯¯ï¼Œä¹Ÿè®©æˆ‘ä»¬åœ¨é‡æ„å¤§å‹é¡¹ç›®çš„æ—¶å€™æ›´æœ‰ä¿¡å¿ƒã€‚é€šè¿‡ IDE ä¸­åŸºäºç±»å‹çš„è‡ªåŠ¨è¡¥å…¨ï¼ŒTypeScript è¿˜æ”¹å–„äº†å¼€å‘ä½“éªŒå’Œæ•ˆç‡ã€‚

x-crawl æœ¬èº«å°±æ˜¯ç”¨ TypeScript ç¼–å†™çš„ï¼Œå¹¶å¯¹ TypeScript æä¾›äº†æ”¯æŒã€‚è‡ªå¸¦ç±»å‹å£°æ˜æ–‡ä»¶ï¼Œå¼€ç®±å³ç”¨ã€‚

## API

### xCrawl

é€šè¿‡è°ƒç”¨ xCrawl åˆ›å»ºä¸€ä¸ªçˆ¬è™«å®ä¾‹ã€‚è¯·æ±‚æ˜¯ç”±å®ä¾‹æ–¹æ³•å†…éƒ¨è‡ªå·±ç»´æŠ¤ï¼Œå¹¶éç”±å®ä¾‹è‡ªå·±ç»´æŠ¤ã€‚

#### ç±»å‹

- [XCrawlBaseConfig](#XCrawlBaseConfig)
- [XCrawlInstance](#XCrawlInstance)

```ts
function xCrawl(baseConfig?: XCrawlBaseConfig): XCrawlInstance
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

// xCrawl API
const myXCrawl = xCrawl({
  baseUrl: 'https://xxx.com',
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})
```

### crawlPage

crawlPage æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºçˆ¬å–é¡µé¢ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [CrawlPageConfig](#CrawlPageConfig) ç±»å‹
- æŸ¥çœ‹ [CrawlPageSingleRes](#CrawlPageSingleRes) ç±»å‹
- æŸ¥çœ‹ [CrawlPageRes](#CrawlPageRes) ç±»å‹

```ts
function crawlPage: <T extends CrawlPageConfig>(
  config: T,
  callback?: ((res: CrawlPageSingleRes) => void) | undefined
) => Promise<CrawlPageRes<T>>
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

// crawlPage API
myXCrawl.crawlPage('https://xxx.com/xxx').then((res) => {
  const { browser, page } = res.data

  // å…³é—­æµè§ˆå™¨
  browser.close()
})
```

#### é…ç½®

ä¸€å…±æœ‰ 4 ç§:

- string
- PageRequestConfig
- (string | PageRequestConfig)[]
- CrawlPageConfigObject

**1.string**

å¦‚æœä½ åªæƒ³å•çº¯çˆ¬ä¸€ä¸‹è¿™ä¸ªé¡µé¢ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlPage('https://xxx.com/xxxx').then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚

**2.PageRequestConfig**

PageRequestConfig çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [PageRequestConfig](#PageRequestConfig) ã€‚

å¦‚æœä½ æƒ³çˆ¬ä¸€ä¸‹è¿™ä¸ªé¡µé¢ï¼Œå¹¶ä¸”éœ€è¦å¤±è´¥é‡è¯•ä¹‹ç±»çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlPage({
    url: 'https://xxx.com/xxxx',
    proxy: 'xxx',
    maxRetry: 1
  })
  .then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚

**3.(string | PageRequestConfig)[]**

PageRequestConfig çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [PageRequestConfig](#PageRequestConfig) ã€‚

å¦‚æœä½ æƒ³çˆ¬å–å¤šä¸ªé¡µé¢ï¼Œå¹¶ä¸”æœ‰äº›é¡µé¢éœ€è¦å¤±è´¥é‡è¯•ä¹‹ç±»çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlPage(['https://xxx.com/xxxx', { url: 'https://xxx.com/xxxx', maxRetry: 2 }])
  .then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢æ˜¯å¯¹è±¡ã€‚

**4.CrawlPageConfigObject**

CrawlPageConfigObject çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [CrawlPageConfigObject](#CrawlPageConfigObject) ã€‚

å¦‚æœä½ æƒ³çˆ¬å–å¤šä¸ªé¡µé¢ï¼Œå¹¶ä¸”è¯·æ±‚é…ç½®ï¼ˆproxyã€cookiesã€é‡è¯•ç­‰ç­‰ï¼‰ä¸æƒ³é‡å¤å†™ï¼Œéœ€è¦é—´éš”æ—¶é—´çš„è¯ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlPage({
  requestConfigs: [
    'https://xxx.com/xxxx',
    { url: 'https://xxx.com/xxxx', maxRetry: 6 }
  ],
  intervalTime: { max: 3000, min: 1000 },
  cookies: 'xxx',
  maxRetry: 1
}).then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢æ˜¯å¯¹è±¡ã€‚

å…³äºç»“æœçš„æ›´å¤šä¿¡æ¯å¯æŸ¥çœ‹ [å…³äºç»“æœ](#å…³äºç»“æœ) ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰ç”¨å³å¯ã€‚

### crawlData

crawl æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºçˆ¬å– API ï¼Œå¯è·å– JSON æ•°æ®ç­‰ç­‰ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [CrawlDataConfig](#CrawlDataConfig) ç±»å‹
- æŸ¥çœ‹ [CrawlDataSingleRes](#CrawlDataSingleRes) ç±»å‹
- æŸ¥çœ‹ [CrawlDataRes](#CrawlDataRes) ç±»å‹

```ts
function crawlData<D = any, T extends CrawlDataConfig = CrawlDataConfig>(
  config: T,
  callback?: ((res: CrawlDataSingleRes<D>) => void) | undefined
) => Promise<CrawlDataRes<D, T>>
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})

// crawlData API
myXCrawl
  .crawlData({
    requestConfigs: ['https://xxx.com/xxxx', 'https://xxx.com/xxxx'],
    intervalTime: { max: 3000, min: 1000 },
    cookies: 'xxx',
    maxRetry: 1
  })
  .then((res) => {
    console.log(res)
  })
```

#### é…ç½®

ä¸€å…±æœ‰ 4 ç§:

- string
- DataRequestConfig
- (string | DataRequestConfig)[]
- CrawlDataConfigObject

**1.string**

å¦‚æœä½ åªæƒ³å•çº¯çˆ¬ä¸€ä¸‹è¿™ä¸ªæ•°æ®ï¼Œå¹¶ä¸”è¯¥æ¥å£æ˜¯ GET æ–¹å¼çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlData('https://xxx.com/xxxx').then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚

**2.DataRequestConfig**

DataRequestConfig çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [DataRequestConfig](#DataRequestConfig) ã€‚

å¦‚æœä½ æƒ³çˆ¬ä¸€ä¸‹è¿™ä¸ªæ•°æ®ï¼Œå¹¶ä¸”éœ€è¦å¤±è´¥é‡è¯•ä¹‹ç±»çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlData({
    url: 'https://xxx.com/xxxx',
    proxy: 'xxx',
    maxRetry: 1
  })
  .then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚

**3.(string | DataRequestConfig)[]**

DataRequestConfig çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [DataRequestConfig](#DataRequestConfig) ã€‚

å¦‚æœä½ æƒ³çˆ¬å–å¤šä¸ªæ•°æ®ï¼Œå¹¶ä¸”æœ‰äº›æ•°æ®éœ€è¦å¤±è´¥é‡è¯•ä¹‹ç±»çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlPage(['https://xxx.com/xxxx', { url: 'https://xxx.com/xxxx', maxRetry: 2 }])
  .then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢æ˜¯å¯¹è±¡ã€‚

**4.CrawlDataConfigObject**

CrawlPageConfigObject çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [CrawlPageConfigObject](#CrawlPageConfigObject) ã€‚

å¦‚æœä½ æƒ³çˆ¬å–å¤šä¸ªæ•°æ®ï¼Œå¹¶ä¸”è¯·æ±‚é…ç½®ï¼ˆproxyã€cookiesã€é‡è¯•ç­‰ç­‰ï¼‰ä¸æƒ³é‡å¤å†™ï¼Œéœ€è¦é—´éš”æ—¶é—´çš„è¯ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlData({
  requestConfigs: [
    'https://xxx.com/xxxx',
    { url: 'https://xxx.com/xxxx', maxRetry: 6 }
  ],
  intervalTime: { max: 3000, min: 1000 },
  cookies: 'xxx',
  maxRetry: 1
}).then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢æ˜¯å¯¹è±¡ã€‚

å…³äºç»“æœçš„æ›´å¤šä¿¡æ¯å¯æŸ¥çœ‹ [å…³äºç»“æœ](#å…³äºç»“æœ) ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰ç”¨å³å¯ã€‚

### crawlFile

crawlFile æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºçˆ¬å–æ–‡ä»¶ï¼Œå¯è·å–å›¾ç‰‡ã€pdf æ–‡ä»¶ç­‰ç­‰ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [CrawlFileConfig](#CrawlFileConfig) ç±»å‹
- æŸ¥çœ‹ [CrawlFileSingleRes](#CrawlFileSingleRes) ç±»å‹
- æŸ¥çœ‹ [CrawlFileRes](#CrawlFileRes) ç±»å‹

```ts
function crawlFile<T extends CrawlFileConfig>(
  config: T,
  callback?: ((res: CrawlFileSingleRes) => void) | undefined
) => Promise<CrawlFileRes<T>>
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})

// crawlFile API
myXCrawl
  .crawlFile({
    requestConfigs: ['https://xxx.com/xxxx', 'https://xxx.com/xxxx'],
    storeDir: './upload',
    intervalTime: { max: 3000, min: 1000 },
    maxRetry: 1
  })
  .then((res) => {})
```

#### é…ç½®

ä¸€å…±æœ‰ 3 ç§:

- FileRequestConfig
- FileRequestConfig[]
- CrawlFileConfigObject

**1.FileRequestConfig**

FileRequestConfig çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [FileRequestConfig](#FileRequestConfig) ã€‚

å¦‚æœä½ æƒ³çˆ¬ä¸€ä¸‹è¿™ä¸ªæ–‡ä»¶ï¼Œå¹¶ä¸”éœ€è¦å¤±è´¥é‡è¯•ä¹‹ç±»çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlFile({
    url: 'https://xxx.com/xxxx',
    proxy: 'xxx',
    maxRetry: 1,
    storeDir: './upload',
    fileName: 'xxx'
  })
  .then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚

**2.FileRequestConfig[]**

FileRequestConfig çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [FileRequestConfig](#FileRequestConfig) ã€‚

å¦‚æœä½ æƒ³çˆ¬å–å¤šä¸ªæ–‡ä»¶ï¼Œå¹¶ä¸”æœ‰äº›æ•°æ®éœ€è¦å¤±è´¥é‡è¯•ä¹‹ç±»çš„ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl
  .crawlFile([
    { url: 'https://xxx.com/xxxx', storeDir: './upload' },
    { url: 'https://xxx.com/xxxx', storeDir: './upload', maxRetry: 2 }
  ])
  .then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢æ˜¯å¯¹è±¡ã€‚

**3.CrawlFileConfigObject**

CrawlFileConfigObject çš„æ›´å¤šé…ç½®é€‰é¡¹å¯ä»¥æŸ¥çœ‹ [CrawlFileConfigObject](#CrawlFileConfigObject) ã€‚

å¦‚æœä½ æƒ³çˆ¬å–å¤šä¸ªæ•°æ®ï¼Œå¹¶ä¸”è¯·æ±‚é…ç½®ï¼ˆstoreDirã€proxyã€é‡è¯•ç­‰ç­‰ï¼‰ä¸æƒ³é‡å¤å†™ï¼Œéœ€è¦é—´éš”æ—¶é—´ç­‰ç­‰çš„è¯ï¼Œå¯ä»¥è¯•è¯•è¿™ç§å†™æ³•ï¼š

```
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl()

myXCrawl.crawlFile({
  requestConfigs: [
    'https://xxx.com/xxxx',
    { url: 'https://xxx.com/xxxx', storeDir: './upload/xxx' }
  ],
  storeDir: './upload',
  intervalTime: { max: 3000, min: 1000 },
  maxRetry: 1
}).then((res) => {})
```

æ‹¿åˆ°çš„ res å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢æ˜¯å¯¹è±¡ã€‚

å…³äºç»“æœçš„æ›´å¤šä¿¡æ¯å¯æŸ¥çœ‹ [å…³äºç»“æœ](#å…³äºç»“æœ) ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰ç”¨å³å¯ã€‚

### startPolling

crawlPolling æ˜¯çˆ¬è™«å®ä¾‹çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºè¿›è¡Œè½®è¯¢æ“ä½œï¼Œæ¯”å¦‚æ¯éš”ä¸€æ®µæ—¶é—´è·å–æ–°é—»ä¹‹ç±»çš„ã€‚

#### ç±»å‹

- æŸ¥çœ‹ [StartPollingConfig](#StartPollingConfig) ç±»å‹

```ts
function startPolling: (
  config: StartPollingConfig,
  callback: (count: number, stopPolling: () => void) => void
) => void
```

#### ç¤ºä¾‹

```js
import xCrawl from 'x-crawl'

const myXCrawl = xCrawl({
  timeout: 10000,
  intervalTime: { max: 2000, min: 1000 }
})

// startPolling API
myXCrawl.startPolling({ h: 2, m: 30 }, (count, stopPolling) => {
  // æ¯éš”ä¸¤ä¸ªåŠå°æ—¶ä¼šæ‰§è¡Œä¸€æ¬¡
  // crawlPage/crawlData/crawlFile
})
```

## ç±»å‹

### API Config

#### API Config Other

##### IntervalTime

```ts
export type IntervalTime = number | { max: number; min?: number }
```

##### Method

```ts
export type Method =
  | 'get'
  | 'GET'
  | 'delete'
  | 'DELETE'
  | 'head'
  | 'HEAD'
  | 'options'
  | 'OPTIONS'
  | 'post'
  | 'POST'
  | 'put'
  | 'PUT'
  | 'patch'
  | 'PATCH'
  | 'purge'
  | 'PURGE'
  | 'link'
  | 'LINK'
  | 'unlink'
  | 'UNLINK'
```

##### PageRequestConfigCookies

```ts
export type PageRequestConfigCookies =
  | string
  | Protocol.Network.CookieParam
  | Protocol.Network.CookieParam[]
```

#### API Config Request

##### PageRequestConfig

```ts
export interface PageRequestConfig {
  url: string
  headers?: AnyObject
  timeout?: number
  proxy?: string
  cookies?: PageRequestConfigCookies
  maxRetry?: number
  priority?: number
}
```

##### DataRequestConfig

```ts
export interface DataRequestConfig {
  url: string
  method?: Method
  headers?: AnyObject
  params?: AnyObject
  data?: any
  timeout?: number
  proxy?: string
  maxRetry?: number
  priority?: number
}
```

##### FileRequestConfig

```ts
export interface FileRequestConfig {
  url: string
  headers?: AnyObject
  timeout?: number
  proxy?: string
  maxRetry?: number
  priority?: number
  storeDir?: string
  fileName?: string
  extension?: string
}
```

#### API Config Crawl

##### XCrawlBaseConfig

```ts
export interface XCrawlBaseConfig {
  baseUrl?: string
  timeout?: number
  intervalTime?: IntervalTime
  mode?: 'async' | 'sync'
  proxy?: string
  maxRetry?: number
}
```

##### CrawlPageConfigObject

```ts
export interface CrawlPageConfigObject {
  requestConfigs: (string | PageRequestConfig)[]
  proxy?: string
  timeout?: number
  cookies?: PageRequestConfigCookies
  intervalTime?: IntervalTime
  maxRetry?: number
}
```

##### CrawlDataConfigObject

```ts
export interface CrawlDataConfigObject {
  requestConfigs: (string | DataRequestConfig)[]
  proxy?: string
  timeout?: number
  intervalTime?: IntervalTime
  maxRetry?: number
}
```

##### CrawlFileConfigObject

```ts
export interface CrawlFileConfigObject {
  requestConfigs: (string | FileRequestConfig)[]
  proxy?: string
  timeout?: number
  intervalTime?: IntervalTime
  maxRetry?: number
  fileConfig?: {
    storeDir?: string
    extension?: string
    beforeSave?: (info: {
      id: number
      fileName: string
      filePath: string
      data: Buffer
    }) => Buffer | void
  }
}
```

##### CrawlPageConfig

```ts
export type CrawlPageConfig =
  | string
  | PageRequestConfig
  | (string | PageRequestConfig)[]
  | CrawlPageConfigObject
```

##### CrawlDataConfig

```ts
export type CrawlDataConfig =
  | string
  | DataRequestConfig
  | (string | DataRequestConfig)[]
  | CrawlDataConfigObject
```

##### CrawlFileConfig

```ts
export type CrawlFileConfig = FileRequestConfig | FileRequestConfig[] | CrawlFileConfigObject
```

##### StartPollingConfig

```js
export interface StartPollingConfig {
  d?: number
  h?: number
  m?: number
}
```

### API Result

#### XCrawlInstance

```ts
export interface XCrawlInstance {
  crawlPage: <T extends CrawlPageConfig>(
    config: T,
    callback?: ((res: CrawlPageSingleRes) => void) | undefined
  ) => Promise<CrawlPageRes<T>>

  crawlData: <D = any, T extends CrawlDataConfig = CrawlDataConfig>(
    config: T,
    callback?: ((res: CrawlDataSingleRes<D>) => void) | undefined
  ) => Promise<CrawlDataRes<D, T>>

  crawlFile: <T extends CrawlFileConfig>(
    config: T,
    callback?: ((res: CrawlFileSingleRes) => void) | undefined
  ) => Promise<CrawlFileRes<T>>

  startPolling: (
    config: StartPollingConfig,
    callback: (count: number, stopPolling: () => void) => void
  ) => void
}
```

#### CrawlCommonRes

```ts
export interface CrawlCommonRes {
  id: number
  isSuccess: boolean
  maxRetry: number
  crawlCount: number
  retryCount: number
  errorQueue: Error[]
}
```

#### CrawlPageSingleRes

```ts
export interface CrawlPageSingleRes extends CrawlCommonRes {
  data: {
    browser: Browser
    response: HTTPResponse | null
    page: Page
  }
}
```

#### CrawlDataSingleRes

```ts
export interface CrawlDataSingleRes<D> extends CrawlCommonRes {
  data: {
    statusCode: number | undefined
    headers: IncomingHttpHeaders
    data: D
  } | null
}
```

#### CrawlFileSingleRes

```ts
export interface CrawlFileSingleRes extends CrawlCommonRes {
  data: {
    statusCode: number | undefined
    headers: IncomingHttpHeaders
    data: {
      isSuccess: boolean
      fileName: string
      fileExtension: string
      mimeType: string
      size: number
      filePath: string
    }
  } | null
}
```

#### CrawlPageRes

```ts
export type CrawlPageRes<R extends CrawlPageConfig> = R extends
  | (string | PageRequestConfig)[]
  | CrawlPageConfigObject
  ? CrawlPageSingleRes[]
  : CrawlPageSingleRes
```

#### CrawlDataRes

```ts
export type CrawlDataRes<D, R extends CrawlDataConfig> = R extends
  | (string | DataRequestConfig)[]
  | CrawlDataConfigObject
  ? CrawlDataSingleRes<D>[]
  : CrawlDataSingleRes<D>
```

#### CrawlFileRes

```ts
export type CrawlFileRes<R extends CrawlFileConfig> = R extends
  | FileRequestConfig[]
  | CrawlFileConfigObject
  ? CrawlFileSingleRes[]
  : CrawlFileSingleRes
```

### API Other

#### AnyObject

```ts
export interface AnyObject extends Object {
  [key: string | number | symbol]: any
}
```

## æ›´å¤š

å¦‚æœæ‚¨æœ‰ **é—®é¢˜ ã€éœ€æ±‚ã€å¥½çš„å»ºè®®** è¯·åœ¨ https://github.com/coder-hxl/x-crawl/issues ä¸­æ **Issues** ã€‚
